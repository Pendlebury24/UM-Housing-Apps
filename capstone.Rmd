---
title: "Capstone"
author: "Ethan Pendlebury"
date: "2024-03-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown



```{r}
library(readxl)
library(rpart)
library(rpart.plot)
library(stats)
library(MASS)
library(janitor)
library(tidyverse)
library(tidymodels)
library(themis)
library(ROSE)
library(caret)

```

```{r}

# JC: Switching to some more standard approaches. Moved the includes up.
data <- read_excel("Data 2019-2023.xlsx") %>% 
  clean_names()

View(data)

```

```{r}
# Cleaning data an removing low frequency values

d <- data %>% 
  filter(m_f != "U", age <= 21,status != "Moved Out", # clean out very low frequency bits
         class != "Graduate",
         academic_year != 2018,
         admit_status !="High School Pilot") %>%
  mutate(
    no_show = as_factor(no_show),
    academic_year = as_factor(academic_year))
    
```

```{r}
#up sampling the data

#creating data set of only variables I want to use for the analysis and upsampling I do not want the 
#upsampling algorithm to accidentally use student ID or other variables that are not relevant to the analysis.
variables_to_keep <- c("m_f", "age", "resident", "month", "campus", "class", "admit_status", "reason", "no_show", "academic_year", "status")
new_d <- d %>% 
  select(all_of(variables_to_keep))

new_d <- new_d %>%
  mutate(no_show = as.factor(no_show)) #changing no_show to a factor



```

Using Random Oversampling Algorithm (ROSE) to balance the data. Chose 11,000 as the number of samples 
because it makes increase the number of no-shows to be about 25% of the data set. 
```{r}

# Adjusting variables to factors for ROSA model
new_d$no_show <- as.factor(new_d$no_show)
new_d$reason <- ifelse(is.na(new_d$reason), "Not Applicable", new_d$reason)
new_d <- new_d %>%
  mutate(across(where(is.character), as.factor))

# Apply ROSE to balance the data
set.seed(123)  # setting seed 
balanced_data <- ovun.sample(no_show ~ ., data = new_d, method = "over", N = 11000)$data

# Check the balanced dataset
head(balanced_data)
table(new_d$no_show)
table(balanced_data$no_show)
```

split data into training and testing sets. I am using a 70/30 split for the training and testing sets.
```{r}

index <- sample(1:nrow(balanced_data), nrow(balanced_data)*0.7)

train_data <- balanced_data[index,]
test_data <- balanced_data[-index,]


```


Logistic Regression Model
```{r}
model <- glm(no_show ~ m_f  + age + resident + month + campus + class + admit_status, 
             data = train_data, family = binomial)

summary(model)
```
```{r}
# Making predictions on the test data
probabilities <- predict(model, newdata = test_data, type = "response")
predicted_classes <- ifelse(probabilities > 0.5, 1, 0)

# Actual outcomes
actual_classes <- test_data$no_show

# Calculating the accuracy
accuracy <- mean(predicted_classes == actual_classes)
print(paste("Accuracy of the model:", accuracy))
```
Using a confusion matrix from carot package to evaluate the model
```{r}
confusionMatrix(as.factor(predicted_classes), as.factor(actual_classes))
```



Decision tree model
```{r}
tree_model <- rpart(no_show ~ m_f + age + month + class + resident + admit_status, data = train_data, method = "class", control = rpart.control(minsplit = 1, minbucket = 250, cp = 0.001))
```


```{r}

# Plot the decision tree
rpart.plot(tree_model, type = 4, extra = 101)
```

```{r}
dec_pred <- predict(tree_model, test_data, type = "class")

accuracy <- sum(dec_pred == test_data$no_show) / nrow(test_data)
print(paste("Accuracy:", accuracy))
```





Older code and comments from the original capstone.Rmd file before adjustments were made based on Johns suggestions.

```{r}
#model <- glm(`No Show` ~ `M/F`   + Age + Resident + Month + Campus + Class, data = train_data, family = binomial)

# JC: I'd drop the "U" on m_f. You've got a TON of missing data in here. I'd consider
# where you could do some imputation. 

model <- glm(no_show ~ m_f  + age + resident + month + campus + class, 
             data = balanced_data, family = binomial)

anova(model,test="Chisq")


```

```{r}
# JC: just doing some exploring

anova(lm(no_show ~ m_f, data=data %>% filter(m_f != "U")))
# I'm shocked this isn't higher. Including "U" is a 

# JC trying to see what's going on to make age such a strong predictor
train_data <- train_data %>% 
  mutate(
    age_grp = case_when(
      age <= 19 ~ "18-19",
      age <= 21 ~ "20-21",
      age <= 25 ~ "22-25",
      TRUE ~ "26+"
    )
  )

summary(lm(no_show ~ age_grp,data=train_data))

# I'm worried that it's being driven by some no-showing olds: 
train_data %>% 
  group_by(age_grp) %>% 
  summarize(
    n=n(),
    num_no_shows=sum(no_show),
    frac_no_show = mean(no_show)
  )
# I think this age relationship is spurious. 

# At this point I switched over to my own r script





```


Chi Square Test
```{r}
contingency_table <- rbind(no_show, applied - no_show)

# Perform the chi-square test for homogeneity
test_result <- chisq.test(contingency_table)

# Output the results
list(
  chi_square_statistic = test_result$statistic,
  p_value = test_result$p.value,
  degrees_of_freedom = test_result$parameter,
  expected_frequencies = test_result$expected
)
```

```{r}
applied <- c(1689, 1510, 1781, 2164, 2298)
no_show <- c(80, 61, 63, 61, 88)

# Function to perform the z-test for two proportions
perform_z_test <- function(n1, x1, n2, x2) {
  p1 <- x1 / n1
  p2 <- x2 / n2
  p_pooled <- (x1 + x2) / (n1 + n2)
  se <- sqrt(p_pooled * (1 - p_pooled) * (1/n1 + 1/n2))
  z <- (p1 - p2) / se
  p_value <- 2 * (1 - pnorm(abs(z)))
  return(c(z = z, p_value = p_value))
}

# Calculate the Bonferroni correction for multiple comparisons
number_of_comparisons <- choose(length(applied), 2)
bonferroni_alpha <- 0.05 / number_of_comparisons

# Perform the Z-tests for each pair of years
results <- data.frame(
  Year1 = integer(),
  Year2 = integer(),
  Z_stat = numeric(),
  P_value = numeric(),
  Significant = logical()
)

# Compare each year with every other year
for (i in 1:(length(applied)-1)) {
  for (j in (i+1):length(applied)) {
    test_result <- perform_z_test(applied[i], no_show[i], applied[j], no_show[j])
    results <- rbind(results, c(2019 + i - 1, 2019 + j - 1, test_result, test_result[2] < bonferroni_alpha))
  }
}

# Rename the columns of the results data frame
colnames(results) <- c("Year1", "Year2", "Z_stat", "P_value", "Significant")

# Output the results sorted by p-value
results[order(results$P_value), ]
```





