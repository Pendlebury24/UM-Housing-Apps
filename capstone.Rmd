---
title: "Capstone"
author: "Ethan Pendlebury"
date: "2024-03-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown



```{r}
library(readxl)
library(rpart)
library(rpart.plot)
library(stats)
library(MASS)
library(janitor)
library(tidyverse)
library(tidymodels)
library(themis)
```

```{r}

# JC: Switching to some more standard approaches. Moved the includes up.
data <- read_excel("Data 2019-2023.xlsx") %>% 
  clean_names()

View(data)

Data <- data # just so I don't have to change all the capital D

summary(Data)
```
Set seed and split data into training and testing sets. I am using a 70/30 split for the training and testing sets.
```{r}

# JC: Seems weird that you're not using all the tidymodels stuff which is designed for this.

set.seed(123)

index <- sample(1:nrow(Data), nrow(Data)*0.7)

train_data <- Data[index,]
test_data <- Data[-index,]

# JC: only 251 noshows in training data. This is probably a case to rebalance
# the data by either upsampling the 1s or using something like SMOTE. You've
# got a model with a 97% accuracy just saying no_show=0

```

Logistic regression model 
```{r}
#model <- glm(`No Show` ~ `M/F`   + Age + Resident + Month + Campus + Class, data = train_data, family = binomial)

# JC: I'd drop the "U" on m_f. You've got a TON of missing data in here. I'd consider
# where you could do some imputation. 

model <- glm(no_show ~ m_f  + age + resident + month + campus + class, 
             data = train_data, family = binomial)

anova(model,test="Chisq")


```

```{r}
# JC: just doing some exploring

anova(lm(no_show ~ m_f, data=data %>% filter(m_f != "U")))
# I'm shocked this isn't higher. Including "U" is a 

# JC trying to see what's going on to make age such a strong predictor
train_data <- train_data %>% 
  mutate(
    age_grp = case_when(
      age <= 19 ~ "18-19",
      age <= 21 ~ "20-21",
      age <= 25 ~ "22-25",
      TRUE ~ "26+"
    )
  )

summary(lm(no_show ~ age_grp,data=train_data))

# I'm worried that it's being driven by some no-showing olds: 
train_data %>% 
  group_by(age_grp) %>% 
  summarize(
    n=n(),
    num_no_shows=sum(no_show),
    frac_no_show = mean(no_show)
  )
# I think this age relationship is spurious. 

# At this point I switched over to my own r script





```




```{r}
summary(model)
```
Current model used for Capstone paper 

```{r}
model2 <- glm(`No Show` ~ `M/F` + Age + Resident + Month + Campus + Class + `Admit Status`, data = train_data, family = binomial)

summary(model2)
```



Data sample of 8 no show graduate students 37 cancellations total small sample size to pull from. 81 graduate in the entire data set.

```{r}
model3 <- glm(`No Show` ~ Class + `Admit Status`, data = train_data, family = binomial)

summary(model3)
```



Changing dependent variable to all students who canceled their application not just those in August

```{r}
model4 <- glm(`Canceled` ~ `M/F` + Age + Resident + Month + Campus + Class + `Admit Status`, data = train_data, family = binomial)

summary(model4)
```






decision tree model
```{r}

tree_model <- rpart(`No Show` ~ `Male` + `Age` + Month + Class, data = train_data, method = "class", control = rpart.control(minsplit = 1, minbucket = 10, cp = 0.001))
```


```{r}

# Plot the decision tree
rpart.plot(tree_model, type = 4, extra = 101)
```



Tree model with more variables 
```{r}

tree_model2 <- rpart(`Canceled` ~ `M/F` + Age + Resident + Month + Campus + Class + `Admit Status` + Reason, data = train_data, method = "class", control = rpart.control(minsplit = 10, minbucket = 100, cp = 0.001))
```
```{r}

# Plot the decision tree
rpart.plot(tree_model2, type = 4, extra = 101)
```

```{r}
contingency_table <- rbind(no_show, applied - no_show)

# Perform the chi-square test for homogeneity
test_result <- chisq.test(contingency_table)

# Output the results
list(
  chi_square_statistic = test_result$statistic,
  p_value = test_result$p.value,
  degrees_of_freedom = test_result$parameter,
  expected_frequencies = test_result$expected
)
```



```{r}
applied <- c(1689, 1510, 1781, 2164, 2298)
no_show <- c(80, 61, 63, 61, 88)

# Function to perform the z-test for two proportions
perform_z_test <- function(n1, x1, n2, x2) {
  p1 <- x1 / n1
  p2 <- x2 / n2
  p_pooled <- (x1 + x2) / (n1 + n2)
  se <- sqrt(p_pooled * (1 - p_pooled) * (1/n1 + 1/n2))
  z <- (p1 - p2) / se
  p_value <- 2 * (1 - pnorm(abs(z)))
  return(c(z = z, p_value = p_value))
}

# Calculate the Bonferroni correction for multiple comparisons
number_of_comparisons <- choose(length(applied), 2)
bonferroni_alpha <- 0.05 / number_of_comparisons

# Perform the Z-tests for each pair of years
results <- data.frame(
  Year1 = integer(),
  Year2 = integer(),
  Z_stat = numeric(),
  P_value = numeric(),
  Significant = logical()
)

# Compare each year with every other year
for (i in 1:(length(applied)-1)) {
  for (j in (i+1):length(applied)) {
    test_result <- perform_z_test(applied[i], no_show[i], applied[j], no_show[j])
    results <- rbind(results, c(2019 + i - 1, 2019 + j - 1, test_result, test_result[2] < bonferroni_alpha))
  }
}

# Rename the columns of the results data frame
colnames(results) <- c("Year1", "Year2", "Z_stat", "P_value", "Significant")

# Output the results sorted by p-value
results[order(results$P_value), ]
```

















